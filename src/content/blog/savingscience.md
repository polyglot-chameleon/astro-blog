---
title: 'Saving Science'
description: 'saving science yeah'
pubDate: 'Oct 23 2024'
heroImage: '/savingscience.jpeg'
---

### 1 Problem statement
Overreliance and outright misuse of null hypothesis significance testing (NHST) in psychological research have been reported extensively. Main reasons commonly involve publication bias lived and enforced by both authors, and reviewers, and the ambiguities afforded by a) researchers’ degrees of freedom (RDOF) [[3](#3)] and/or b) vague exploratory rather than organized confirmatory behavior [[5](#5), [7](#7)]

Practices are manifold and manifest themselves generally under what is commonly referred to as *p-hacking*  in the broader sense of the term (according to [[6](#6)]: *e.g. selective reporting of the dependent and/or independent variable, optional stopping
(aka data peeking), outlier exclusion, covariate inclusion, (questionnaire) scale redefinition, favourable imputation [[6](#6)], all the way to outright data fabrication (wakefield, stapel,
lesn´e))*).

With the immediate result of a replication crisis that paved the way for potentially comedic consequences, e.g. the case of an allegedly
significant precognition effect. [[1](#1), [7](#7), [5](#5)]

Ultimately, the described problem might be symptomatic of a fundamental mismatch between epistemic and methodical capacities and the very ontic structure of the psyche. [[4](#4)]

---

### 2 Suggested improvement

#### 2.1 Methodical
While it is true that aiming at more rigid standards for e.g. (higher) sample sizes, (lower) significance levels in general increases the importance, and the
pursuit of complementary approaches and measures, e.g effect sizes [[6](#6)], bayes factors [7](#7) provides for ”cross-methodical” validation of observed statistical
effects, these measures *per se* do not protect against questionable research practices (QRP) and underlying intentions in general [[5](#5), [6](#6)].

More importantly, authors shall be disincentivized of any kind of manipulative ”embellishment” of their data and conclusions drawn from it. Authors might therefore consider e.g. 1) clearly defining their rule for ter-
minating data collection before the collection even begins. [[5](#5), [7](#7)], 2) listing all (in/dependent) variables and experimental conditions, including a) failed manipulations b) eliminated obversations / outliers and their impact and c) arbitrarily included covariates. [[5](#5)]

#### 2.2 Institutional
Protective measures are futile if not enforced by the respective authorities and/or gateway keepers who might consider establishing standards against described self-serving practices. For instance, requiring a) preregistration at the journal in order to control for RDOF [6](#6)] and b) authors to conduct exact replications in case of questionable results. [[5](#5)]

#### 2.3 Economical
We claim that it is ultimately a matter of incentives which are mostly dictated by the political & industrial framework (the proverbial funding agency manager) that enables any scholarly enterprise. A compromise shall be
sought to accomodate all parties in this equation (industry, public institutions, research).

---

### 3 Conclusion
We close this discussion with an intriguing argument by [[4](#4)] who, leaning on the shoulders of science philosopher Thomas Kuhn, suggests that with the
observed replication crisis we might be witnessing a potential model drift – when anomalies have become the rule – and ultimately hints at a looming
paradigm shift / model revolution in psychological research. [[4](#4), p.4] [[2](#2)] set an optimistic outlook on the parameters of this paradigm shift that might have already borne its fruits.


### References

<a target="blank_" id=1 href="https://pubmed.ncbi.nlm.nih.gov/21280961/">[1] D. J. Bem. Feeling the future: Experimental evidence for anomalous retroactive influences on cognition and affect. Journal of Personality and Social Psychology, 100(3):407–425, 2011</a>

<a target="blank_" id=2 href="https://www.nature.com/articles/s44271-023-00003-2">[2] M. Korbmacher, F. Azevedo, C. R. Pennington, H. Hartmann, M. Pownall, K. Schmidt, M. Elsherif, N. Breznau, O. Robertson, T. Kalandadze, S. Yu, B. J. Baker, A. O’Mahony, J. O. S. Olsnes, J. J. Shaw,B. Gjoneska, Y. Yamada, J. P. Röer, J. Murphy, S. Alzahawi, S. Grinschgl, C. M. Oliveira, T. Wingen, S. K. Yeung, M. Liu, L. M. König, N. Albayrak-Aydemir, O. Lecuona, L. Micheli, and T. Evans. The replication crisis has led to positive structural, procedural, and community changes. Communications Psychology, 1(1), July 2023.</a>

<a target="blank_" id=3 href="https://journals.sagepub.com/doi/10.1080/17470218.2012.711335">[3] E. Masicampo and D. R. Lalande. A peculiar prevalence of p values just below .05. Quarterly Journal of Experimental Psychology, 65(11):2271–2279, Nov. 2012.</a>

<a target="blank_" id=4 href="https://pubmed.ncbi.nlm.nih.gov/39328812/">[4] R. Mayrhofer, I. C. Büchner, and J. Hevesi. The quantitative paradigm and the nature of the human mind. the replication crisis as an epistemological crisis of quantitative psychology in view of the ontic nature of the psyche. Frontiers in Psychology, 15, Sept. 2024.</a>

<a target="blank_" id=5 href="https://journals.sagepub.com/doi/full/10.1177/0956797611417632">[5] J. P. Simmons, L. D. Nelson, and U. Simonsohn. False-positive psychology: Undisclosed flexibility in data collection and analysis allows
presenting anything as significant. Psychological Science, 22(11):1359–1366, Oct. 2011.</a>

<a target="blank_" id=6 href="https://royalsocietypublishing.org/doi/10.1098/rsos.220346">[6] A. M. Stefan and F. D. Schönbrodt. Big little lies: a compendium and simulation of p-hacking strategies. Royal Society Open Science, 10(2), Feb. 2023.</a>

<a target="blank_" id=7 href="https://pubmed.ncbi.nlm.nih.gov/21280965/">[7] E.-J. Wagenmakers, R. Wetzels, D. Borsboom, and H. L. J. van der
Maas. Why psychologists must change the way they analyze their data:
The case of psi: Comment on bem (2011). Journal of Personality and
Social Psychology, 100(3):426–432, Mar. 2011.3</a>
